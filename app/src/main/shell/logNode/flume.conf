# node config, name node
# every node log file have two config, one is for the logs that need write into hdfs, the other one is for the logs that need write into hbase
node.sources = hdfssrc1 hdfssrc2 hbasesrc1 hbasesrc2
# two channel, same as sources
node.channels = hdfschannel hbasechannel
# collector sinks
node.sinks = hdfssink1 hdfssink2 hbasesink1 hbasesink2
# sink groups, fail-over and load-balance
node.sinkgroups = hdfssinkgroup hbasesinkgroup

# Describe the source for all logs
node.sources.hdfssrc1.type = exec
node.sources.hdfssrc1.command = tail -fn0 /usr/local/nginx-1.4.4/logs/www.example.com_access.log
node.sources.hdfssrc1.interceptors = hdfsInterceptor
node.sources.hdfssrc1.interceptors.hdfsInterceptor.type = com.iuni.data.analyze.flume.IuniLogInterceptor$Builder
node.sources.hdfssrc1.interceptors.hdfsInterceptor.preserveExisting = true
node.sources.hdfssrc1.interceptors.hdfsInterceptor.useIP = false
node.sources.hdfssrc1.interceptors.hdfsInterceptor.hostHeader = hostname
node.sources.hdfssrc1.interceptors.hdfsInterceptor.regex = ^(.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)$
node.sources.hdfssrc1.interceptors.hdfsInterceptor.requestTimeIndex = 3

node.sources.hdfssrc2.type = exec
node.sources.hdfssrc2.command = tail -fn0 /usr/local/nginx-1.4.4/logs/www.example.com_access.log
node.sources.hdfssrc2.interceptors = hdfsInterceptor
node.sources.hdfssrc2.interceptors.hdfsInterceptor.type = com.iuni.data.analyze.flume.IuniLogInterceptor$Builder
node.sources.hdfssrc2.interceptors.hdfsInterceptor.preserveExisting = true
node.sources.hdfssrc2.interceptors.hdfsInterceptor.useIP = false
node.sources.hdfssrc2.interceptors.hdfsInterceptor.hostHeader = hostname
node.sources.hdfssrc2.interceptors.hdfsInterceptor.regex = ^(.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)$
node.sources.hdfssrc2.interceptors.hdfsInterceptor.requestTimeIndex = 3

# Describe the source for usefull logs
node.sources.hbasesrc1.type = exec
node.sources.hbasesrc1.command = tail -fn0 /usr/local/nginx-1.4.4/logs/www.example.com_access.log
node.sources.hbasesrc1.interceptors = hbaseInterceptor
node.sources.hbasesrc1.interceptors.hbaseInterceptor.type = com.iuni.data.analyze.flume.IuniLogInterceptor$Builder
node.sources.hbasesrc1.interceptors.hbaseInterceptor.preserveExisting = true
node.sources.hbasesrc1.interceptors.hbaseInterceptor.useIP = false
node.sources.hbasesrc1.interceptors.hbaseInterceptor.hostHeader = hostname
node.sources.hbasesrc1.interceptors.hbaseInterceptor.regex = ^(.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)$
node.sources.hbasesrc1.interceptors.hbaseInterceptor.urlIndex = 7
node.sources.hbasesrc1.interceptors.hbaseInterceptor.statusIndex = 8
node.sources.hbasesrc1.interceptors.hbaseInterceptor.statusError = false
node.sources.hbasesrc1.interceptors.hbaseInterceptor.requestTimeIndex = 3
node.sources.hbasesrc1.interceptors.hbaseInterceptor.statusRegex = ^2[0-9]{2}$
node.sources.hbasesrc1.interceptors.hbaseInterceptor.static = false
node.sources.hbasesrc1.interceptors.hbaseInterceptor.staticRes = css js jpg jpeg png gif ico img bmp min small
node.sources.hbasesrc1.interceptors.hbaseInterceptor.jpg.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.jpeg.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.png.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.gif.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.ico.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.img.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.bmp.specialUrl = specialUrl
node.sources.hbasesrc1.interceptors.hbaseInterceptor.bmp.specialUrl = specialUrl

node.sources.hbasesrc2.type = exec
node.sources.hbasesrc2.command = tail -fn0 /usr/local/nginx-1.4.4/logs/www.example.com_access.log
node.sources.hbasesrc2.interceptors = hbaseInterceptor
node.sources.hbasesrc2.interceptors.hbaseInterceptor.type = com.iuni.data.analyze.flume.IuniLogInterceptor$Builder
node.sources.hbasesrc2.interceptors.hbaseInterceptor.preserveExisting = true
node.sources.hbasesrc2.interceptors.hbaseInterceptor.useIP = false
node.sources.hbasesrc2.interceptors.hbaseInterceptor.hostHeader = hostname
node.sources.hbasesrc2.interceptors.hbaseInterceptor.regex = ^(.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)\\{\\](.*)$
node.sources.hbasesrc2.interceptors.hbaseInterceptor.urlIndex = 7
node.sources.hbasesrc2.interceptors.hbaseInterceptor.statusIndex = 8
node.sources.hbasesrc2.interceptors.hbaseInterceptor.statusError = false
node.sources.hbasesrc2.interceptors.hbaseInterceptor.requestTimeIndex = 3
node.sources.hbasesrc2.interceptors.hbaseInterceptor.statusRegex = ^2[0-9]{2}$
node.sources.hbasesrc2.interceptors.hbaseInterceptor.static = false
node.sources.hbasesrc2.interceptors.hbaseInterceptor.staticRes = css js jpg jpeg png gif ico img bmp min small
node.sources.hbasesrc2.interceptors.hbaseInterceptor.jpg.specialUrl = specialUrl
node.sources.hbasesrc2.interceptors.hbaseInterceptor.jpeg.specialUrl = specialUrl
node.sources.hbasesrc2.interceptors.hbaseInterceptor.png.specialUrl = specialUrl
node.sources.hbasesrc2.interceptors.hbaseInterceptor.gif.specialUrl = specialUrl
node.sources.hbasesrc2.interceptors.hbaseInterceptor.ico.specialUrl = specialUrl
node.sources.hbasesrc2.interceptors.hbaseInterceptor.img.specialUrl = specialUrl
node.sources.hbasesrc2.interceptors.hbaseInterceptor.bmp.specialUrl = specialUrl

# Use a channel which buffers events in memory
node.channels.hdfschannel.type = memory
node.channels.hdfschannel.keep-alive = 30
node.channels.hdfschannel.capacity = 500000
node.channels.hdfschannel.transactionCapacity = 5000
node.channels.hdfschannel.byteCapacity = 50000000

node.channels.hbasechannel.type = memory
node.channels.hbasechannel.keep-alive = 30
node.channels.hbasechannel.capacity = 500000
node.channels.hbasechannel.transactionCapacity = 5000
node.channels.hbasechannel.byteCapacity = 50000000

# Describe the avro sink
node.sinks.hdfssink1.type=avro
node.sinks.hdfssink1.hostname = 127.0.0.1
node.sinks.hdfssink1.port = 4141
node.sinks.hdfssink1.batch-size = 1000

node.sinks.hdfssink2.type=avro
node.sinks.hdfssink2.hostname = 127.0.0.1
node.sinks.hdfssink2.port = 4141
node.sinks.hdfssink2.batch-size = 1000

node.sinks.hbasesink1.type=avro
node.sinks.hbasesink1.hostname = 127.0.0.1
node.sinks.hbasesink1.port = 4142
node.sinks.hbasesink1.batch-size = 1000

node.sinks.hbasesink2.type=avro
node.sinks.hbasesink2.hostname = 127.0.0.1
node.sinks.hbasesink2.port = 4142
node.sinks.hbasesink2.batch-size = 1000

# Bind the source and sink to the channel
node.sources.hdfssrc1.channels = hdfschannel
node.sources.hdfssrc2.channels = hdfschannel
node.sinks.hdfssink1.channel = hdfschannel
node.sinks.hdfssink2.channel = hdfschannel

node.sources.hbasesrc1.channels = hbasechannel
node.sources.hbasesrc2.channels = hbasechannel
node.sinks.hbasesink1.channel = hbasechannel
node.sinks.hbasesink2.channel = hbasechannel

# Describe group sinks
node.sinkgroups.hdfssinkgroup.sinks = hdfssink1 hdfssink2
node.sinkgroups.hdfssinkgroup.processor.type = load_balance
node.sinkgroups.hdfssinkgroup.processor.selector = round_robin
node.sinkgroups.hdfssinkgroup.processor.backoff = true

node.sinkgroups.hbasesinkgroup.sinks = hbasesink1 hbasesink2
node.sinkgroups.hbasesinkgroup.processor.type = load_balance
node.sinkgroups.hbasesinkgroup.processor.selector = round_robin
node.sinkgroups.hbasesinkgroup.processor.backoff = true

# collector config, name collector
collector.sources = hdfssrc hbasesrc
collector.sinks = hdfssink hbasesink
collector.channels = hdfschannel hbasechannel

# Describe/configure the source
collector.sources.hdfssrc.type = avro
collector.sources.hdfssrc.bind = 0.0.0.0
collector.sources.hdfssrc.port = 4141

collector.sources.hbasesrc.type = avro
collector.sources.hbasesrc.bind = 0.0.0.0
collector.sources.hbasesrc.port = 4142

# Describe the hdfds sink
collector.sinks.hdfssink.type = hdfs
collector.sinks.hdfssink.hdfs.path = hdfs://physical.hadoop:8020/flume/test/%Y%m%d/
collector.sinks.hdfssink.hdfs.fileType = DataStream
collector.sinks.hdfssink.hdfs.filePrefix = iuni-
collector.sinks.hdfssink.hdfs.fileSuffix = .log
collector.sinks.hdfssink.hdfs.round = true
collector.sinks.hdfssink.hdfs.roundValue = 24
collector.sinks.hdfssink.hdfs.roundUnit = hour
collector.sinks.hdfssink.hdfs.batchSize = 100
collector.sinks.hdfssink.hdfs.useLocalTimeStamp = true
collector.sinks.hdfssink.hdfs.rollInterval = 0
collector.sinks.hdfssink.hdfs.rollSize = 64000000
collector.sinks.hdfssink.hdfs.rollCount = 0
collector.sinks.hdfssink.hdfs.batchSize = 10000
collector.sinks.hdfssink.serializer = com.iuni.data.analyze.flume.IuniLogTextSerializer$Builder

# Describe the hbase sink
# collector.sinks.hbasesink.type = asynchbase
collector.sinks.hbasesink.type = hbase
collector.sinks.hbasesink.table = tc
collector.sinks.hbasesink.columnFamily = fu
collector.sinks.hbasesink.batchSize = 10000
# collector.sinks.hbasesink.serializer = com.iuni.data.analyze.flume.IuniLogAsyncHbaseSerializer
collector.sinks.hbasesink.serializer = com.iuni.data.analyze.flume.IuniLogHbaseSerializer
collector.sinks.hbasesink.serializer.colNames = c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17

# Use a channel which buffers events in memory
collector.channels.hdfschannel.type = memory
collector.channels.hdfschannel.keep-alive = 30
collector.channels.hdfschannel.capacity = 500000
collector.channels.hdfschannel.transactionCapacity = 5000
collector.channels.hdfschannel.byteCapacity = 50000000

collector.channels.hbasechannel.type = memory
collector.channels.hbasechannel.keep-alive = 30
collector.channels.hbasechannel.capacity = 500000
collector.channels.hbasechannel.transactionCapacity = 5000
collector.channels.hbasechannel.byteCapacity = 50000000

# Bind the source and sink to the channel
collector.sources.hdfssrc.channels = hdfschannel
collector.sinks.hdfssink.channel = hdfschannel

collector.sources.hbasesrc.channels = hbasechannel
collector.sinks.hbasesink.channel = hbasechannel